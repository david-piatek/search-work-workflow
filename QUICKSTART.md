# Quick Start Guide

## Installation Rapide (5 minutes)

### 1. Pr√©requis

Installer si n√©cessaire:
```bash
# Node.js 18+
brew install node

# pnpm
npm install -g pnpm

# Task
brew install go-task

# Docker Desktop
# T√©l√©charger depuis https://www.docker.com/products/docker-desktop
```

### 2. Installation du Projet

```bash
# Cloner
git clone <votre-repo>
cd searc-work-workflow

# Installer les d√©pendances
pnpm install

# Configurer l'environnement
cp apps/backend/.env.example apps/backend/.env
```

### 3. Lancer en D√©veloppement

```bash
# Option 1: Tout avec Docker
task docker:up

# Option 2: Dev local (plus rapide)
# Terminal 1: D√©marrer PostgreSQL + Redis
docker-compose up -d postgres redis

# Terminal 2: Backend
cd apps/backend
pnpm dev

# Terminal 3: Frontend
cd apps/frontend
pnpm dev
```

### 4. Acc√©der √† l'Application

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:3000
- **Swagger**: http://localhost:3000/api

## Premier Scraping

1. Ouvrir http://localhost:5173
2. Cliquer sur "freework" ou un autre site
3. Cliquer sur "Start Scraping"
4. Attendre 30 secondes
5. Voir les r√©sultats s'afficher

## Commandes Essentielles

```bash
# D√©veloppement
task dev                # Lance tout

# Tests
task test              # Tous les tests
task test:e2e          # Tests e2e uniquement

# Build
task build             # Build prod

# Docker
task docker:up         # Lance avec Docker
task docker:down       # Arr√™te Docker
task docker:logs       # Voir les logs

# Documentation
task storybook         # Composants UI
task swagger           # API docs (ou http://localhost:3000/api)

# Nettoyage
task clean            # Nettoie tout
```

## Structure du Code

```
apps/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ scrapers/        ‚Üê Logique de scraping
‚îÇ       ‚îú‚îÄ‚îÄ jobs/            ‚Üê Gestion des jobs scrap√©s
‚îÇ       ‚îî‚îÄ‚îÄ main.ts
‚îî‚îÄ‚îÄ frontend/
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ components/      ‚Üê Composants Svelte
        ‚îú‚îÄ‚îÄ lib/             ‚Üê API client
        ‚îî‚îÄ‚îÄ App.svelte
```

## Ajouter un Nouveau Site

### M√©thode 1: Interface Web

1. Entrer l'URL dans "Or enter custom URL..."
2. Cliquer "Start Scraping"
3. Le scraper est g√©n√©r√© automatiquement

### M√©thode 2: API

```bash
curl -X POST http://localhost:3000/scrapers \
  -H "Content-Type: application/json" \
  -d '{
    "name": "nouveau-site",
    "url": "https://example.com/jobs"
  }'
```

## D√©ploiement Rapide

### Docker Compose

```bash
# Build
task docker:build

# Deploy
task docker:up

# Acc√©der
open http://localhost:5173
```

### Kubernetes + Helm

```bash
# Build images
docker build -t job-scraper-backend:latest -f apps/backend/Dockerfile .
docker build -t job-scraper-frontend:latest -f apps/frontend/Dockerfile .

# Deploy
helm install job-scraper ./helm/job-scraper-app \
  --namespace job-scraper \
  --create-namespace
```

### ArgoCD (GitOps)

```bash
# Pousser vers GitHub
git remote add origin <your-repo>
git push -u origin main

# Configurer ArgoCD
kubectl apply -f argocd/application.yaml

# ArgoCD synchronise automatiquement
```

## Workflow n8n

1. **Installer n8n** (si pas d√©j√† fait):
```bash
docker run -it --rm \
  --name n8n \
  -p 5678:5678 \
  -v ~/.n8n:/home/node/.n8n \
  n8nio/n8n
```

2. **Importer le workflow**:
   - Ouvrir http://localhost:5678
   - Workflows ‚Üí Import from File
   - S√©lectionner `n8n/job-scraper-workflow.json`

3. **Configurer**:
   - GitHub token pour backup
   - Slack webhook (optionnel)
   - Activer le workflow

## Troubleshooting Rapide

### Port d√©j√† utilis√©
```bash
# Trouver et tuer le processus
lsof -ti:3000 | xargs kill -9
lsof -ti:5173 | xargs kill -9
```

### Base de donn√©es ne d√©marre pas
```bash
docker-compose down -v
docker-compose up -d postgres
```

### D√©pendances manquantes
```bash
pnpm install --force
```

### Reset complet
```bash
task clean
rm -rf node_modules apps/*/node_modules
pnpm install
```

## Documentation Compl√®te

- **README principal**: Voir [README.md](README.md)
- **Helm/K8s**: Voir [argocd/README.md](argocd/README.md)
- **n8n Workflow**: Voir [n8n/README.md](n8n/README.md)
- **CLAUDE.md**: Documentation technique interne

## Support

- Issues GitHub pour bugs
- README.md pour documentation compl√®te
- Swagger pour API reference
- Storybook pour composants UI

## Next Steps

Une fois l'app lanc√©e:

1. Tester le scraping sur diff√©rents sites
2. Explorer l'API Swagger
3. Voir les composants dans Storybook
4. Configurer le workflow n8n
5. D√©ployer sur Kubernetes
6. Configurer ArgoCD pour GitOps

Bon scraping ! üöÄ
